{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ObJAX_CIFAR10_example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxqOyLRaUkYW"
      },
      "source": [
        "# ObJAX CIFAR10 example\n",
        "\n",
        "It is recommended to run this notebook on GPU. In Google Colab this could be set through `Runtime -> Change runtime type` menu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYU-k0VsVRbL"
      },
      "source": [
        "# Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dg5WFQApM1_"
      },
      "source": [
        "%pip --quiet install objax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oizwGGdIUMr0"
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jn\n",
        "from jax.lax import lax\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import objax\n",
        "from objax.zoo.wide_resnet import WideResNet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ65n3dQVvll"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnzqFRPpVwZ5"
      },
      "source": [
        "base_learning_rate = 0.1 # Learning rate\n",
        "lr_decay_epochs = 30     # How often to decay learning rate\n",
        "lr_decay_factor = 0.2    # By how much to decay learning rate\n",
        "weight_decay =  0.0005   # Weight decay\n",
        "batch_size = 128         # Batch size\n",
        "num_train_epochs = 100   # Number of training epochs\n",
        "wrn_width = 2            # Width of WideResNet\n",
        "wrn_depth = 28           # Depth of WideResNet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu5ildnhWF3M"
      },
      "source": [
        "# Setup dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBQ-xNSDWIsj"
      },
      "source": [
        "# Augmentation function for input data\n",
        "def augment(x):  # x is NCHW\n",
        "  \"\"\"Random flip and random shift augmentation of image batch.\"\"\"\n",
        "  if random.random() < .5:\n",
        "    x = x[:, :, :, ::-1]  # Flip the batch images about the horizontal axis\n",
        "  # Pixel-shift all images in the batch by up to 4 pixels in any direction.\n",
        "  x_pad = np.pad(x, [[0, 0], [0, 0], [4, 4], [4, 4]], 'reflect')\n",
        "  rx, ry = np.random.randint(0, 4), np.random.randint(0, 4)\n",
        "  x = x_pad[:, :, rx:rx + 32, ry:ry + 32]\n",
        "  return x\n",
        "\n",
        "# Data\n",
        "data = tfds.as_numpy(tfds.load(name='cifar10', batch_size=-1))\n",
        "x_train = data['train']['image'].transpose(0, 3, 1, 2) / 255.0\n",
        "y_train = data['train']['label']\n",
        "x_test = data['test']['image'].transpose(0, 3, 1, 2) / 255.0\n",
        "y_test = data['test']['label']\n",
        "del data\n",
        "\n",
        "# Model\n",
        "model = WideResNet(nin=3, nclass=10, depth=wrn_depth, width=wrn_width)\n",
        "model_vars = model.vars()\n",
        "weight_decay_vars = [v for k, v in model_vars.items() if k.endswith('.w')]\n",
        "\n",
        "# Optimizer\n",
        "opt = objax.optimizer.Momentum(model_vars, nesterov=True)\n",
        "\n",
        "# Prediction operation\n",
        "predict_op = lambda x: objax.functional.softmax(model(x, training=False))\n",
        "predict_op = objax.Jit(predict_op, model_vars)\n",
        "\n",
        "# Loss and training op\n",
        "def loss_fn(x, label):\n",
        "  logit = model(x, training=True)\n",
        "  xe_loss = objax.functional.loss.cross_entropy_logits_sparse(logit, label).mean()\n",
        "  wd_loss = sum((v.value ** 2).sum() for v in weight_decay_vars)\n",
        "  return xe_loss + weight_decay * wd_loss\n",
        "\n",
        "loss_gv = objax.GradValues(loss_fn, model.vars())\n",
        "\n",
        "def train_op(x, y, learning_rate):\n",
        "    grads, loss = loss_gv(x, y)\n",
        "    opt(learning_rate, grads)\n",
        "    return loss\n",
        "\n",
        "all_vars = model_vars + opt.vars()\n",
        "train_op = objax.Jit(train_op, all_vars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6KUOcDxC7gq"
      },
      "source": [
        "**Model parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMzCIgeJC-Wu",
        "outputId": "346b5364-0a56-4099-bbb9-78fcba908311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model_vars)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(WideResNet)[0](Conv2D).w                                        432 (3, 3, 3, 16)\n",
            "(WideResNet)[1](WRNBlock).proj_conv(Conv2D).w                    512 (1, 1, 16, 32)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).running_mean        16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).running_var         16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).beta                16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_1(BatchNorm2D).gamma               16 (1, 16, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).conv_1(Conv2D).w                      4608 (3, 3, 16, 32)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[1](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[2](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[3](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).conv_1(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).norm_2(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[4](WRNBlock).conv_2(Conv2D).w                      9216 (3, 3, 32, 32)\n",
            "(WideResNet)[5](WRNBlock).proj_conv(Conv2D).w                   2048 (1, 1, 32, 64)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).running_mean        32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).running_var         32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).beta                32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_1(BatchNorm2D).gamma               32 (1, 32, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).conv_1(Conv2D).w                     18432 (3, 3, 32, 64)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[5](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[6](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[7](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).conv_1(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).norm_2(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[8](WRNBlock).conv_2(Conv2D).w                     36864 (3, 3, 64, 64)\n",
            "(WideResNet)[9](WRNBlock).proj_conv(Conv2D).w                   8192 (1, 1, 64, 128)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).running_mean        64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).running_var         64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).beta                64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_1(BatchNorm2D).gamma               64 (1, 64, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).conv_1(Conv2D).w                     73728 (3, 3, 64, 128)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).running_mean       128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).running_var        128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).beta               128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).norm_2(BatchNorm2D).gamma              128 (1, 128, 1, 1)\n",
            "(WideResNet)[9](WRNBlock).conv_2(Conv2D).w                    147456 (3, 3, 128, 128)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[10](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[11](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_1(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).conv_1(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).running_mean      128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).running_var       128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).beta              128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).norm_2(BatchNorm2D).gamma             128 (1, 128, 1, 1)\n",
            "(WideResNet)[12](WRNBlock).conv_2(Conv2D).w                   147456 (3, 3, 128, 128)\n",
            "(WideResNet)[13](BatchNorm2D).running_mean                       128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).running_var                        128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).beta                               128 (1, 128, 1, 1)\n",
            "(WideResNet)[13](BatchNorm2D).gamma                              128 (1, 128, 1, 1)\n",
            "(WideResNet)[16](Linear).b                                        10 (10,)\n",
            "(WideResNet)[16](Linear).w                                      1280 (128, 10)\n",
            "+Total(130)                                                  1471226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQH7UyxxWf9j"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgnrJyv0Whah",
        "outputId": "bcb5a5eb-8c0f-4b42-bf24-fe35a1611097",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "  return base_learning_rate * math.pow(lr_decay_factor, epoch // lr_decay_epochs)\n",
        "\n",
        "num_train_examples = x_train.shape[0]\n",
        "num_test_examples = x_test.shape[0]\n",
        "for epoch in range(num_train_epochs):\n",
        "  # Training\n",
        "  example_indices = np.arange(num_train_examples)\n",
        "  np.random.shuffle(example_indices)\n",
        "  for idx in range(0, num_train_examples, batch_size):\n",
        "    x = x_train[example_indices[idx:idx + batch_size]]\n",
        "    y = y_train[example_indices[idx:idx + batch_size]]\n",
        "    loss = train_op(augment(x), y, lr_schedule(epoch))[0]\n",
        "\n",
        "  # Eval\n",
        "  accuracy = 0\n",
        "  for idx in range(0, num_test_examples, batch_size):\n",
        "    x = x_test[idx:idx + batch_size]\n",
        "    y = y_test[idx:idx + batch_size]\n",
        "    p = predict_op(x)\n",
        "    accuracy += (np.argmax(p, axis=1) == y).sum()\n",
        "  accuracy /= num_test_examples\n",
        "  print(f'Epoch {epoch+1:3} -- train loss {loss:.3f}   test accuracy {accuracy*100:.1f}', flush=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1 -- train loss 2.059   test accuracy 35.4\n",
            "Epoch   2 -- train loss 1.291   test accuracy 70.2\n",
            "Epoch   3 -- train loss 1.266   test accuracy 65.7\n",
            "Epoch   4 -- train loss 1.005   test accuracy 67.2\n",
            "Epoch   5 -- train loss 0.909   test accuracy 74.4\n",
            "Epoch   6 -- train loss 0.808   test accuracy 76.3\n",
            "Epoch   7 -- train loss 0.813   test accuracy 71.0\n",
            "Epoch   8 -- train loss 0.770   test accuracy 78.8\n",
            "Epoch   9 -- train loss 0.701   test accuracy 77.2\n",
            "Epoch  10 -- train loss 0.788   test accuracy 74.7\n",
            "Epoch  11 -- train loss 0.834   test accuracy 74.5\n",
            "Epoch  12 -- train loss 1.031   test accuracy 77.7\n",
            "Epoch  13 -- train loss 0.889   test accuracy 77.8\n",
            "Epoch  14 -- train loss 0.697   test accuracy 75.3\n",
            "Epoch  15 -- train loss 0.669   test accuracy 75.8\n",
            "Epoch  16 -- train loss 0.643   test accuracy 80.4\n",
            "Epoch  17 -- train loss 0.840   test accuracy 80.2\n",
            "Epoch  18 -- train loss 0.779   test accuracy 78.7\n",
            "Epoch  19 -- train loss 0.768   test accuracy 78.8\n",
            "Epoch  20 -- train loss 0.947   test accuracy 73.3\n",
            "Epoch  21 -- train loss 0.665   test accuracy 75.2\n",
            "Epoch  22 -- train loss 0.784   test accuracy 80.5\n",
            "Epoch  23 -- train loss 0.977   test accuracy 74.0\n",
            "Epoch  24 -- train loss 0.701   test accuracy 63.2\n",
            "Epoch  25 -- train loss 0.722   test accuracy 71.3\n",
            "Epoch  26 -- train loss 0.714   test accuracy 75.8\n",
            "Epoch  27 -- train loss 0.840   test accuracy 78.8\n",
            "Epoch  28 -- train loss 0.792   test accuracy 76.5\n",
            "Epoch  29 -- train loss 0.913   test accuracy 80.7\n",
            "Epoch  30 -- train loss 0.760   test accuracy 75.1\n",
            "Epoch  31 -- train loss 0.505   test accuracy 89.6\n",
            "Epoch  32 -- train loss 0.460   test accuracy 89.8\n",
            "Epoch  33 -- train loss 0.532   test accuracy 88.9\n",
            "Epoch  34 -- train loss 0.478   test accuracy 88.9\n",
            "Epoch  35 -- train loss 0.437   test accuracy 88.7\n",
            "Epoch  36 -- train loss 0.463   test accuracy 89.7\n",
            "Epoch  37 -- train loss 0.415   test accuracy 89.2\n",
            "Epoch  38 -- train loss 0.499   test accuracy 88.2\n",
            "Epoch  39 -- train loss 0.499   test accuracy 88.0\n",
            "Epoch  40 -- train loss 0.460   test accuracy 89.5\n",
            "Epoch  41 -- train loss 0.491   test accuracy 86.7\n",
            "Epoch  42 -- train loss 0.353   test accuracy 88.4\n",
            "Epoch  43 -- train loss 0.448   test accuracy 88.5\n",
            "Epoch  44 -- train loss 0.383   test accuracy 88.3\n",
            "Epoch  45 -- train loss 0.353   test accuracy 87.2\n",
            "Epoch  46 -- train loss 0.465   test accuracy 86.9\n",
            "Epoch  47 -- train loss 0.467   test accuracy 87.9\n",
            "Epoch  48 -- train loss 0.411   test accuracy 86.3\n",
            "Epoch  49 -- train loss 0.449   test accuracy 88.1\n",
            "Epoch  50 -- train loss 0.533   test accuracy 87.8\n",
            "Epoch  51 -- train loss 0.591   test accuracy 85.6\n",
            "Epoch  52 -- train loss 0.557   test accuracy 88.6\n",
            "Epoch  53 -- train loss 0.452   test accuracy 88.4\n",
            "Epoch  54 -- train loss 0.473   test accuracy 87.9\n",
            "Epoch  55 -- train loss 0.472   test accuracy 90.0\n",
            "Epoch  56 -- train loss 0.478   test accuracy 88.3\n",
            "Epoch  57 -- train loss 0.376   test accuracy 89.4\n",
            "Epoch  58 -- train loss 0.354   test accuracy 88.9\n",
            "Epoch  59 -- train loss 0.604   test accuracy 88.1\n",
            "Epoch  60 -- train loss 0.505   test accuracy 88.5\n",
            "Epoch  61 -- train loss 0.282   test accuracy 92.3\n",
            "Epoch  62 -- train loss 0.303   test accuracy 92.7\n",
            "Epoch  63 -- train loss 0.265   test accuracy 92.4\n",
            "Epoch  64 -- train loss 0.243   test accuracy 92.6\n",
            "Epoch  65 -- train loss 0.230   test accuracy 92.2\n",
            "Epoch  66 -- train loss 0.253   test accuracy 92.1\n",
            "Epoch  67 -- train loss 0.216   test accuracy 92.5\n",
            "Epoch  68 -- train loss 0.246   test accuracy 92.6\n",
            "Epoch  69 -- train loss 0.214   test accuracy 92.4\n",
            "Epoch  70 -- train loss 0.201   test accuracy 92.6\n",
            "Epoch  71 -- train loss 0.207   test accuracy 92.5\n",
            "Epoch  72 -- train loss 0.197   test accuracy 92.6\n",
            "Epoch  73 -- train loss 0.193   test accuracy 92.6\n",
            "Epoch  74 -- train loss 0.201   test accuracy 92.7\n",
            "Epoch  75 -- train loss 0.244   test accuracy 92.3\n",
            "Epoch  76 -- train loss 0.175   test accuracy 92.5\n",
            "Epoch  77 -- train loss 0.218   test accuracy 92.5\n",
            "Epoch  78 -- train loss 0.180   test accuracy 92.5\n",
            "Epoch  79 -- train loss 0.158   test accuracy 92.6\n",
            "Epoch  80 -- train loss 0.159   test accuracy 92.4\n",
            "Epoch  81 -- train loss 0.166   test accuracy 92.1\n",
            "Epoch  82 -- train loss 0.148   test accuracy 92.4\n",
            "Epoch  83 -- train loss 0.209   test accuracy 92.0\n",
            "Epoch  84 -- train loss 0.210   test accuracy 92.1\n",
            "Epoch  85 -- train loss 0.148   test accuracy 91.5\n",
            "Epoch  86 -- train loss 0.139   test accuracy 91.9\n",
            "Epoch  87 -- train loss 0.159   test accuracy 92.0\n",
            "Epoch  88 -- train loss 0.133   test accuracy 92.1\n",
            "Epoch  89 -- train loss 0.161   test accuracy 92.0\n",
            "Epoch  90 -- train loss 0.140   test accuracy 91.6\n",
            "Epoch  91 -- train loss 0.134   test accuracy 92.8\n",
            "Epoch  92 -- train loss 0.129   test accuracy 93.1\n",
            "Epoch  93 -- train loss 0.126   test accuracy 93.1\n",
            "Epoch  94 -- train loss 0.125   test accuracy 93.3\n",
            "Epoch  95 -- train loss 0.136   test accuracy 93.3\n",
            "Epoch  96 -- train loss 0.128   test accuracy 93.3\n",
            "Epoch  97 -- train loss 0.126   test accuracy 93.2\n",
            "Epoch  98 -- train loss 0.122   test accuracy 93.3\n",
            "Epoch  99 -- train loss 0.132   test accuracy 93.2\n",
            "Epoch 100 -- train loss 0.122   test accuracy 93.4\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}